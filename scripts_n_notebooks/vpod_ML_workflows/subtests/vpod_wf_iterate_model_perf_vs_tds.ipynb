{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21fdea4",
   "metadata": {},
   "source": [
    "# <font color=green>deepBreaks Applications</font>\n",
    "## Modeling spectral tuning sites of opsin proteins based on amino-acid sequence...  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d1bf1",
   "metadata": {},
   "source": [
    "# <font color=red>Performance vs. Training Data Size Subtest</font>\n",
    "### **Output** = a set of models trained on smaller and smaller amounts of the training data and a file tracking the change in performance to use for graph/figure making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb85989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing deepBreaks libraries \n",
    "from deepBreaks.utils import get_models, get_scores, get_params, make_pipeline\n",
    "from deepBreaks.preprocessing import MisCare, ConstantCare, URareCare, CustomOneHotEncoder\n",
    "from deepBreaks.preprocessing import FeatureSelection, CollinearCare\n",
    "from deepBreaks.preprocessing import read_data\n",
    "from deepBreaks.models import model_compare_cv, finalize_top, importance_from_pipe, mean_importance, summarize_results\n",
    "from deepBreaks.visualization import plot_scatter, dp_plot, plot_imp_model, plot_imp_all\n",
    "from deepBreaks.preprocessing import write_fasta\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f75803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining user params, file pathes, analysis type\n",
    "\n",
    "#assign your path to folder containing all the datasplits\n",
    "path = 'c:/Users/safra/Documents/GitHub/visual-physiology-opsin-db/vpod_data/VPOD_1.0/formatted_data_splits/vpod_2023-10-16_12-13-11'\n",
    "# path to sequences of interest\n",
    "seqFileName = f'{path}/VPOD_wds_het_1.0.fasta' \n",
    "# path to corresponding metadata of interest\n",
    "metaDataFileName = f'{path}/wds_meta.tsv' \n",
    "\n",
    "# name of the phenotype\n",
    "mt = 'Lambda_Max'\n",
    "\n",
    "# type of the sequences\n",
    "seq_type = 'aa'\n",
    "\n",
    "# type of the analysis if it is a classification model, then we put cl instead of reg\n",
    "ana_type = 'reg' \n",
    "\n",
    "gap_threshold = 0.60\n",
    "\n",
    "#Whether or not you want to drop the reference sequence from the training data- Usually 'Bovine' or 'Squid'\n",
    "drop_ref = True\n",
    "\n",
    "print('reading meta-data')\n",
    "# importing metadata\n",
    "meta_data = read_data(metaDataFileName, seq_type = None, is_main=False)\n",
    "metaFile = metaDataFileName.split('/')[1]\n",
    "# importing sequences data\n",
    "print('reading fasta file')\n",
    "ref_df = read_data(seqFileName, seq_type = seq_type, is_main=True, gap_threshold=gap_threshold)\n",
    "#merging in lambda max values, simultaneously dropping all sequences without entries in metadata file\n",
    "ref_df= ref_df.merge(meta_data.loc[:, mt],  left_index=True, right_index=True)\n",
    "#tr.shape\n",
    "\n",
    "first_run = 0\n",
    "stop_marker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15687869",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = read_data(seqFileName, seq_type = seq_type, is_main=True, gap_threshold=gap_threshold)\n",
    "#merging in lambda max values, simultaneously dropping all sequences without entries in metadata file\n",
    "full_df= full_df.merge(meta_data.loc[:, mt],  left_index=True, right_index=True)\n",
    "full_df.drop(mt, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f084f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "while stop_marker == 0:\n",
    "    tr = read_data(seqFileName, seq_type = seq_type, is_main=True, gap_threshold=gap_threshold)\n",
    "    #merging in lambda max values, simultaneously dropping all sequences without entries in metadata file\n",
    "    tr = tr.merge(meta_data.loc[:, mt],  left_index=True, right_index=True)\n",
    "    #tr.shape\n",
    "    # making a unique directory for saving the reports of the analysis\n",
    "    print('direcory preparation')\n",
    "    dt_label = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    seqFile = seqFileName.split('/')[2]\n",
    "    print(seqFile)\n",
    "    seqFile = seqFile.split('.')[0]\n",
    "    print(seqFile)\n",
    "    report_dir = str(seqFile +'_' + mt + '_' + dt_label)\n",
    "    os.makedirs(report_dir)\n",
    "\n",
    "    #changing the number of test sequences to extract from the training dataframe based on the size of the training dataset\n",
    "    if tr.shape[0] <= 150:\n",
    "        sample_n = 15\n",
    "    elif tr.shape[0] > 150 and tr.shape[0] <= 300:\n",
    "        sample_n = 15\n",
    "    elif tr.shape[0] > 300 and tr.shape[0] <= 1000:\n",
    "        sample_n = 15\n",
    "    else:\n",
    "        sample_n = 100\n",
    "\n",
    "    #taking a sample of 'n' sequences from the reference dataframe and dropping the selected sequences so they don't get resampled\n",
    "    n=0\n",
    "    while n<1:\n",
    "        #try:\n",
    "            if first_run == 0:\n",
    "                drop_indices = np.random.choice(ref_df.index, sample_n, replace=False)\n",
    "                #print(drop_indices)\n",
    "                ref_df = ref_df.drop(drop_indices)\n",
    "                n+=1\n",
    "                first_run+=1\n",
    "\n",
    "            else:\n",
    "                new_drop_indices = np.random.choice(ref_df.index, sample_n, replace=False)\n",
    "                ref_df = ref_df.drop(new_drop_indices)\n",
    "                drop_indices = np.append(drop_indices, new_drop_indices)\n",
    "            \n",
    "                print(f'There are {ref_df.shape[0]} sequences remaining in the training data')\n",
    "                if (int(ref_df.shape[0]) - sample_n) < 30:\n",
    "                    stop_marker+=1\n",
    "                n+=1\n",
    "        #except:\n",
    "            #print('Sequence sampling error')\n",
    "    #dropping our sample indices from the training dataframe\n",
    "    tr = tr.drop(drop_indices)\n",
    "\n",
    "    y = tr.loc[:, mt].values\n",
    "    tr.drop(mt, axis=1, inplace=True)\n",
    "\n",
    "    #settingthe paramaters for our ML pipeline\n",
    "    prep_pipeline = make_pipeline(\n",
    "        steps=[\n",
    "            ('mc', MisCare(missing_threshold=0.05)),\n",
    "            ('cc', ConstantCare()),\n",
    "            ('ur', URareCare(threshold=0.025)),\n",
    "            ('cc2', ConstantCare()),\n",
    "            ('one_hot', CustomOneHotEncoder()),\n",
    "            ('feature_selection', FeatureSelection(model_type=ana_type, alpha=0.10, keep=False)),\n",
    "            ('collinear_care', CollinearCare(dist_method='correlation', threshold=0.05, keep=False))\n",
    "        ])\n",
    "\n",
    "    #training models\n",
    "    report, top = model_compare_cv(X=tr, y=y, preprocess_pipe=prep_pipeline,\n",
    "                                models_dict=get_models(ana_type=ana_type),\n",
    "                                scoring=get_scores(ana_type=ana_type),\n",
    "                                report_dir=report_dir,\n",
    "                                cv=12, ana_type=ana_type, cache_dir=report_dir)\n",
    "\n",
    "    model_report_file = f\"./{report_dir}/model_report.csv\"\n",
    "    model_report = read_data(model_report_file, seq_type = None, is_main=False)\n",
    "    perf_v_seqs = open(\"perf_vs_seqs.tsv\", 'a')\n",
    "    if first_run == 1:\n",
    "        perf_v_seqs.write(f\"# of Sequences\\tR2\\n\")\n",
    "        first_run+=1  \n",
    "    perf_v_seqs.write(ref_df.shape[0] + '\\t' + model_report['R2'][0] + '\\n')\n",
    "\n",
    "    time.sleep(1)\n",
    "    #setting parameters for tuning the top 3 performing models\n",
    "    prep_pipeline = make_pipeline(\n",
    "        steps=[\n",
    "            ('mc', MisCare(missing_threshold=0.05)),\n",
    "            ('cc', ConstantCare()),\n",
    "            ('ur', URareCare(threshold=0.025)),\n",
    "            ('cc2', ConstantCare()),\n",
    "            ('one_hot', CustomOneHotEncoder()),\n",
    "            ('feature_selection', FeatureSelection(model_type=ana_type, alpha=0.10, keep=True)),\n",
    "            ('collinear_care', CollinearCare(dist_method='correlation', threshold=0.05, keep=True))\n",
    "        ])\n",
    "\n",
    "    modified_top = []\n",
    "    mtml = []\n",
    "    for model in top:\n",
    "        modified_top.append(make_pipeline(steps=[('prep', prep_pipeline), model.steps[-1]]))\n",
    "        my_top_models = str(model[1:])\n",
    "        #print(my_top_models)\n",
    "        my_top_models = my_top_models.split(\"'\")[3]\n",
    "        mtml.append(my_top_models)\n",
    "        #print(my_top_models)\n",
    "\n",
    "    #print(mtml)\n",
    "    time.sleep(1)\n",
    "\n",
    "    #tuning the top 3 performing models \n",
    "    top = finalize_top(X=tr, y=y, top_models=modified_top, grid_param=get_params(),report_dir=report_dir, cv=10)\n",
    "    #summarize the results by extracting feature importance and p-values and grouping correlated features.\n",
    "    sr = summarize_results(top_models=top, report_dir=report_dir)\n",
    "    #plot a scatter plot with -log of (p-value) column as the x-axis and the values of the other columns \n",
    "    scatter_plot = plot_scatter(summary_result=sr, report_dir=report_dir)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    #plot mean relative importance of each feature - corresponding to an amino acid position.\n",
    "    mean_imp = mean_importance(top, report_dir=report_dir)\n",
    "\n",
    "    dp_plot(importance=mean_imp,imp_col='mean', model_name='mean', report_dir=report_dir)\n",
    "    tr = prep_pipeline[:4].fit_transform(tr)\n",
    "\n",
    "    for model in top:\n",
    "        model_name = model.steps[-1][0]\n",
    "        dp_plot(importance=importance_from_pipe(model),\n",
    "                imp_col='standard_value',\n",
    "                model_name = model_name, report_dir=report_dir)\n",
    "        \n",
    "        plot_imp_model(importance=importance_from_pipe(model), \n",
    "                X_train=tr, y_train=y, model_name=model_name,\n",
    "                    meta_var='meta', model_type=ana_type, report_dir=report_dir)\n",
    "\n",
    "    time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepBreaks_altenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "1566c31c90cd32f196ccfa15812cd8e8608767ea4549b0419bac2fea141e189a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
